const express = require('express');
const cors = require('cors');
const axios = require('axios');

const app = express();
// Google Cloud Runì˜ ê¸°ë³¸ í¬íŠ¸(8080)ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
const PORT = process.env.PORT || 3000;

const NIM_API_BASE = process.env.NIM_API_BASE || 'https://integrate.api.nvidia.com/v1';
const NIM_API_KEY = process.env.NIM_API_KEY;

const SHOW_REASONING = false;
const ENABLE_THINKING_MODE = false;

// ðŸ”´ [í•µì‹¬ ìˆ˜ì •] ëª¨ë¸ ë§¤í•‘ í…Œì´ë¸” (ê³¨ë¼ ì“°ê¸° ê°€ëŠ¥)
const MODEL_MAPPING = {
  // 1. ë©”ì¸ ì¶”ì²œ: Llama 3.1 405B (ë…¼ë¦¬ì™•, ì•ˆì •ì„± ìµœê³ , ìž‘ê°€ë‹˜ ë´‡ ìµœì í™”)
  // ì œë‹ˆí„°ì—ì„œ 'gpt-4o' ë˜ëŠ” 'gpt-4'ë¥¼ ì„ íƒí•˜ë©´ ì´ê²Œ ë‚˜ì˜µë‹ˆë‹¤.
  'gpt-4o': 'meta/llama-3.1-405b-instruct',
  'gpt-4': 'meta/llama-3.1-405b-instruct',

  // 2. ì„œë¸Œ ì¶”ì²œ: DeepSeek V3 (ê°ì„±ì™•, í•„ë ¥ ì¢‹ìŒ, ëœ ê±´ì¡°í•¨)
  // ì œë‹ˆí„°ì—ì„œ 'gpt-4-turbo'ë¥¼ ì„ íƒí•˜ë©´ ì´ê²Œ ë‚˜ì˜µë‹ˆë‹¤.
  // *ì£¼ì˜: R1ì´ ì•„ë‹ˆë¼ V3ë¼ì„œ ë‚œìˆ˜ ì•ˆ í„°ì§‘ë‹ˆë‹¤.
  'gpt-4-turbo': 'deepseek-ai/deepseek-v3',

  // 3. ì†ë„ìš©: Llama 3.1 70B (ê°€ë³ê³  ë¹ ë¦„)
  // ì œë‹ˆí„°ì—ì„œ 'gpt-3.5-turbo'ë¥¼ ì„ íƒí•˜ë©´ ì´ê²Œ ë‚˜ì˜µë‹ˆë‹¤.
  'gpt-3.5-turbo': 'meta/llama-3.1-70b-instruct',

  // 4. ê¸°íƒ€ í˜¸í™˜ì„± (SillyTavern ë“± ë‹¤ë¥¸ íˆ´ì„ ìœ„í•´ ë‚¨ê²¨ë‘ )
  'claude-3-opus': 'meta/llama-3.1-405b-instruct',
  'claude-3-sonnet': 'meta/llama-3.1-70b-instruct',
  'gemini-pro': 'deepseek-ai/deepseek-v3'
};

app.use(cors());
app.use(express.json());

app.get('/health', (req, res) => {
  res.json({
    status: 'ok',
    service: 'OpenAI to NVIDIA NIM Proxy',
    reasoning_display: SHOW_REASONING,
    thinking_mode: ENABLE_THINKING_MODE
  });
});

app.get('/v1/models', (req, res) => {
  const models = Object.keys(MODEL_MAPPING).map(model => ({
    id: model,
    object: 'model',
    created: Date.now(),
    owned_by: 'nvidia-nim-proxy'
  }));

  res.json({
    object: 'list',
    data: models
  });
});

app.post('/v1/chat/completions', async (req, res) => {
  try {
    const { model, messages, temperature, max_tokens, stream } = req.body;

    let nimModel = MODEL_MAPPING[model];
    
    // ë§¤í•‘ëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ Llama 405B ì‚¬ìš© (ì•ˆì „ìž¥ì¹˜)
    if (!nimModel) {
       nimModel = 'meta/llama-3.1-405b-instruct';
    }

    const nimRequest = {
      model: nimModel,
      messages: messages,
      temperature: temperature || 0.6,
      max_tokens: max_tokens || 1024, // ê¸°ë³¸ í† í° ë„‰ë„‰í•˜ê²Œ
      extra_body: ENABLE_THINKING_MODE ? { chat_template_kwargs: { thinking: true } } : undefined,
      stream: stream || false
    };

    const response = await axios.post(`${NIM_API_BASE}/chat/completions`, nimRequest, {
      headers: {
        'Authorization': `Bearer ${NIM_API_KEY}`,
        'Content-Type': 'application/json'
      },
      responseType: stream ? 'stream' : 'json'
    });

    if (stream) {
      res.setHeader('Content-Type', 'text/event-stream');
      res.setHeader('Cache-Control', 'no-cache');
      res.setHeader('Connection', 'keep-alive');

      response.data.pipe(res); // ìŠ¤íŠ¸ë¦¼ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬ (ë³µìž¡í•œ ë¡œì§ ì œê±°í•˜ì—¬ ì•ˆì •ì„± í™•ë³´)
      
    } else {
      // Non-streaming ì‘ë‹µ ì²˜ë¦¬
      res.json(response.data);
    }

  } catch (error) {
    console.error('Proxy error:', error.message);
    if (error.response) {
        console.error('Error details:', error.response.data);
    }

    res.status(error.response?.status || 500).json({
      error: {
        message: error.message || 'Internal server error',
        type: 'invalid_request_error',
        code: error.response?.status || 500
      }
    });
  }
});

app.all('*', (req, res) => {
  res.status(404).json({
    error: {
      message: `Endpoint ${req.path} not found`,
      type: 'invalid_request_error',
      code: 404
    }
  });
});

// 0.0.0.0ìœ¼ë¡œ ë°”ì¸ë”©í•˜ì—¬ ì™¸ë¶€ ì ‘ì† í—ˆìš© (êµ¬ê¸€ í´ë¼ìš°ë“œ í•„ìˆ˜)
app.listen(PORT, '0.0.0.0', () => {
  console.log(`OpenAI to NVIDIA NIM Proxy running on port ${PORT}`);
  console.log(`Health check: http://localhost:${PORT}/health`);
});
